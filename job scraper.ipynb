{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24460d7e-5f70-4528-95b9-4bf159f7a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd80339-4a0a-41ca-97df-8d333efab431",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = '''\n",
    "Apple\n",
    "Airbnb\n",
    "Twitch\n",
    "Servicenow\n",
    "Stripe\n",
    "Slack\n",
    "Okta, inc.\n",
    "Microsoft\n",
    "Tesla\n",
    "Instacart\n",
    "Square\n",
    "Netapp\n",
    "Visa\n",
    "Affirm, Inc.\n",
    "Paypal\n",
    "Autodesk\n",
    "Quora\n",
    "Snap Inc.\n",
    "Workday\n",
    "Opendoor.Com\n",
    "Cerner Corporation\n",
    "Yelp\n",
    "Zillow\n",
    "Sofi\n",
    "Roblox\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1aa909f-dce8-4a75-9a1b-279e63db973b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple',\n",
       " 'Airbnb',\n",
       " 'Twitch',\n",
       " 'Servicenow',\n",
       " 'Stripe',\n",
       " 'Slack',\n",
       " 'Okta, inc.',\n",
       " 'Microsoft',\n",
       " 'Tesla',\n",
       " 'Instacart',\n",
       " 'Square',\n",
       " 'Netapp',\n",
       " 'Visa',\n",
       " 'Affirm, Inc.',\n",
       " 'Paypal',\n",
       " 'Autodesk',\n",
       " 'Quora',\n",
       " 'Snap Inc.',\n",
       " 'Workday',\n",
       " 'Opendoor.Com',\n",
       " 'Cerner Corporation',\n",
       " 'Yelp',\n",
       " 'Zillow',\n",
       " 'Sofi',\n",
       " 'Roblox']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies.strip('\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0c645-0af4-4284-b191-5b3706d08927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c86ec91d-c5a4-4dd0-ab42-76b7061f9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_urls = {\n",
    "    # 'Faire': 'https://boards.greenhouse.io/faire',\n",
    "    'Point72': 'https://boards.greenhouse.io/point72',\n",
    "    'Twitch': 'https://boards.greenhouse.io/twitch',\n",
    "    # 'Faire': 'https://boards.greenhouse.io/embed/job_board?for=airbnb',\n",
    "    'Ogilvy Health USA': 'https://boards.greenhouse.io/ogilvyhealthusa',\n",
    "    'Ogilvy UK': 'https://boards.greenhouse.io/ogilvyuk'\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aaddbe7-c811-4a2a-90a3-bdb325627242",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3002110595.py, line 109)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 109\u001b[0;36m\u001b[0m\n\u001b[0;31m    if\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def load_page(url):\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 ' + \\\n",
    "        '(KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'\n",
    "    response = requests.get(url, headers={'User-Agent': user_agent})\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_greenhouse_page(response):\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    jobs = []\n",
    "    for div in soup.find_all('div', {'class':'opening'}):\n",
    "        job_title = div.find('a').text\n",
    "        job_url = 'https://boards.greenhouse.io' + div.find('a')['href']\n",
    "        job_loc = div.find('span').text\n",
    "        element = div\n",
    "        hierarchy = []\n",
    "        while element and (element.name != 'section' or 'level-0' not in element.get('class', [])):\n",
    "            element = element.parent\n",
    "            hierarchy.append(element.find_all()[0].get_text(separator=' '))\n",
    "        jobs.append({\n",
    "            'Title': job_title,\n",
    "            'URL': job_url,\n",
    "            'Location': job_loc,\n",
    "            'Hierarchy': hierarchy\n",
    "        })\n",
    "    return jobs\n",
    "\n",
    "def fwprint(*things, w=14, **kwargs):\n",
    "    trunc_things = [str(thing)[:w] for thing in things]\n",
    "    fw_things = [thing+(' '*(w-len(thing))) for thing in trunc_things]\n",
    "    print(*fw_things, **kwargs)\n",
    "    \n",
    "def save_to_db(company_name, jobs, scraped_at):\n",
    "    print()\n",
    "    fwprint('Title', 'Company', 'Location', 'Department', 'Scraped_at', 'URL', sep=' | ')\n",
    "    fwprint(*['-'*12]*6, sep=' | ')\n",
    "    for job in jobs:\n",
    "        title = job['Title']\n",
    "        location = job['Location']\n",
    "        department = job['Hierarchy']\n",
    "        url = job['URL']\n",
    "        \n",
    "        fwprint(title, company_name, location, department, scraped_at, url, sep=' | ')\n",
    "    print()\n",
    "\n",
    "\n",
    "def update_status(timestamp, id, task, scraper_type, company, site, url):\n",
    "    fwprint(timestamp, '01', task, scraper_type, company, site, url, sep=' | ')\n",
    "\n",
    "\n",
    "def scrape_company_page(company_name, company_page_url, jobs_site='Greenhouse'):\n",
    "    fwprint('timestamp', 'id', 'task', 'scraper_type', 'company', 'site', 'url', sep=' | ')\n",
    "    fwprint(*['-'*12]*7, sep=' | ')\n",
    "\n",
    "    scraped_at = time.time()\n",
    "    update_status(\n",
    "        timestamp=scraped_at,\n",
    "        id='01',\n",
    "        task='Scraping page',\n",
    "        scraper_type='company',\n",
    "        company=company_name,\n",
    "        site=jobs_site,\n",
    "        url=company_page_url\n",
    "    ) # initial task\n",
    "    response = load_page(company_page_url)\n",
    "\n",
    "    parsed_at = time.time()\n",
    "    update_status(\n",
    "        timestamp=parsed_at,\n",
    "        id='01',\n",
    "        task='Parsing page',\n",
    "        scraper_type='company',\n",
    "        company=company_name,\n",
    "        site=jobs_site,\n",
    "        url=company_page_url\n",
    "    ) # parsing\n",
    "    if jobs_site=='Greenhouse':\n",
    "        jobs = parse_greenhouse_page(response)\n",
    "\n",
    "    stored_at = time.time()\n",
    "    update_status(\n",
    "        timestamp=stored_at,\n",
    "        id='01',\n",
    "        task='Storing jobs in DB',\n",
    "        scraper_type='company',\n",
    "        company=company_name,\n",
    "        site=jobs_site,\n",
    "        url=company_page_url\n",
    "    ) # saving to db\n",
    "    save_to_db(company_name, jobs, scraped_at)\n",
    "\n",
    "    finished_at = time.time()\n",
    "    update_status(\n",
    "        timestamp=finished_at,\n",
    "        id='01',\n",
    "        task='Done',\n",
    "        scraper_type='company',\n",
    "        company=company_name,\n",
    "        site=jobs_site,\n",
    "        url=company_page_url\n",
    "    ) # done\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_company_page('Twitch', 'https://boards.greenhouse.io/twitch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b575a35-2949-4daf-9ff3-8f28d85c81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for company_name, company_page_url in gh_urls.items():\n",
    "    scrape_company_page(company_name, company_page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8528ccd-bfe6-4e09-af37-ceab39ac4cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\tthere\thello\n"
     ]
    }
   ],
   "source": [
    "print('hi', 'there', 'hello', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a2b61-2964-4783-9c80-c8c51fb5b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a80971c-ed90-4309-b2bf-c4b802b97b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi          \tthere       \thello       \n"
     ]
    }
   ],
   "source": [
    "fwprint('hi', 'there', 'hello', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59910aef-3687-40af-b156-92723fd9aebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b567bd50-1bb5-4b79-84a9-a4fed8b3643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f587f23d-5c97-43a9-85c5-f8e54128ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://0ozmeqv5h5.execute-api.us-east-1.amazonaws.com/default/hey-lambda?paroooometer=Sargam&param2=Kevin,joe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "edb044e5-a73a-4de5-b480-57f3ceabe6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)#, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db8b7b86-7eb6-4eaf-8c72-35d4a803be72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'param2': 'Kevin,joe', 'paroooometer': 'Sargam'}\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec209b0-9117-4e07-8bbc-828d15bbad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://0ozmeqv5h5.execute-api.us-east-1.amazonaws.com/live/scrape-company-jobs-page'\n",
    "params={key: value}\n",
    "\n",
    "?site=greenhouse&url=https://boards.greenhouse.io/point72'\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f5f225-2fcd-487e-977b-1b538ed29ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator:\n",
    "\n",
    "workers come to the orch for direction\n",
    "\n",
    "orch spins up workers\n",
    "orch makes a note of which worker came to it and what task it gave to that worker:\n",
    "\n",
    "    SCRAPER_STATUS\n",
    "        timestamp   | id  | task                   | sc_type | company | site | url\n",
    "        ==============================================================================================================================\n",
    "        69865238946 | 03  | started scraping       | company | point72 | gnhs | https://boards.greenhouse.io/point72\n",
    "        69524397867 | 01  | finished scraping      | company | ogilvy  | gnhs | https://boards.greenhouse.io/ogilvyuk\n",
    "        69865238946 | 02  | pushing jobs to db     | company | meta    | -    | -\n",
    "        69865238946 | 02  | pushed jobs to db      | company | meta    | -    | -\n",
    "        # 57824968598 | 05  | idle                   | company | -       | -    | -\n",
    "        69865238946 | 103 | started scraping       | job     | point72 | gnhs | https://boards.greenhouse.io/ogilvyuk/jobs/4298945005\n",
    "        69524397867 | 101 | finished scraping      | job     | ogilvy  | gnhs | https://boards.greenhouse.io/ogilvyuk/jobs/1698943094\n",
    "        69865238946 | 102 | pushing job data to db | job     | meta    | gnhs | https://boards.greenhouse.io/ogilvyuk/jobs/5326623322\n",
    "        69865238946 | 102 | pushed job data to db  | job     | meta    | gnhs | https://boards.greenhouse.io/ogilvyuk/jobs/7534352334\n",
    "\n",
    "they leave with a task - status scraping\n",
    "they check back in when they finish parsing\n",
    "then they store the data in the bd\n",
    "and say when they're done with that\n",
    "then they're idle\n",
    "and then they ask for more work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896dcee1-8f0f-478a-8998-0cc55a0ef5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "greenhouse-monitor",
   "language": "python",
   "name": "greenhouse-monitor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
